---
layout: post
title:  "What is PCA and how to do it in R ?"
date:   2018-12-25
categories: jekyll update
---



<div style="text-align: justify">

The principal component analysis (PCA) is a very powerful tool that can be used for multiple things. We will see in this post its most common uses.

</div>

## Principal component analysis

<div style="text-align: justify">
<p>
The goal of a PCA is to change the system of coordinate into a different one in order to reduce the number of dimensions of the problem, without losing (too much) information. To make it simple, the PCA relies on the diagonalization of the covariance matrix. The eigen vectors give us the new system of coordinate and the eigen values give us a way to choose the dimension of the reduced problem thanks to some rules that I will not explain here. There are some tools that can help you know how much information you lost during this transformation. Before explaining how it works in R, I will remind you that it is very important to <b>normalize</b> (and clean) the data set before working on it. It is not always mandatory to normalize the data set, but it won’t hurt to do so !
</p>

<p>
The package “FactoMineR” in R is very useful to do PCA on data sets. It is important to look at the index of the qualitative variables and specify them in the function PCA, otherwise they can produce an error. After a call to this function, you can see the eigen values and choose the number of dimension for the problem. Here is a picture of what you can get as a result.
</p>
</div>

<p align="center">
	<img src="/images/eigenvalue.png" width="350">
</p>

<div style="text-align: justify">
<p>
Maybe the simplest and most known rule to choose the dimension of the problem, is to choose the number of eigen values bigger than 1. So here this problem of dimension 10, could be reduce to a problem of dimension 4.
</p>
<p>
Using the function "summary()", you can see variance explained by the 10 new axis. It can also be a way to choose the number of axis you want to keep for your reduced problem. Keep in mind that the goal is to reduce the dimension.
</p>

<p align="center">
	<img src="/images/cumulvar.png" width="550">
</p>
<p>
What you are looking for is the cumulative percentage of variance. The closer it is to 100%, the more information you explain with the axis. It can also be a way to choose the number of axis to keep.
</p>

<p>
When you use the PCA function by "FactoMineR", you also get the correlation circle. This is also a useful tool to see different things. First, you can see the variance explained by the first two axis. Each arrow represents a variable. The closer they are to the edge of the circle, the better they are explained by the two axis. Note that all those interpretations are available if the variables are well explained by the two axis. You can also identify independant variables. If two arrows make a 90° angle, they are most probably independant. Finally, if several arrows are "packed", it means that the associated variables are correlated.
</p>

<p align="center">
	<img src="/images/cercle.png" width="550">
</p>

<p>
To finish with this post, here is an example taken from wikipedia to illustrate the PCA. On a multivariate Gaussian distribution centered at (1,3) with a standard deviation of 3 in the (0.866,0.5) direction, and of 1 in the orthogonal direction, the computation of the two axis with this method gives us the two axis as you can see below. Those two axis give us the best way to explain the dispersion of the points.
</p>
<p align="center">
	<img src="/images/PCA-gaussian.png" width="350">
</p>

</div>

## Conclusion

<div style="text-align: justify">

To conclude with, PCA is a very useful tool to understand how the variables in a data set are linked, and how you can reduce the dimension of a problem, without losing too much information. Just keep in mind that the data has to be normalized and clean before doing the PCA.

</div>

